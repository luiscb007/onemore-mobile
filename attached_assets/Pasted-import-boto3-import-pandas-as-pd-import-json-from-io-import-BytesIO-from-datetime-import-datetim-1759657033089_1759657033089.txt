import boto3
import pandas as pd
import json
from io import BytesIO
from datetime import datetime
from decimal import Decimal
import re
import uuid
import math

def decimal_default(obj):
    if isinstance(obj, Decimal):
        return float(obj)
    raise TypeError

def find_best_asin_matches(keyword, asins_data, batch_size=1000, max_matches=5):
    """Find best matching ASINs for a keyword using batched processing"""
    keyword_lower = keyword.lower()
    keyword_terms = set(re.findall(r'\w+', keyword_lower))
    
    matches = []
    
    # Process ASINs in batches
    for i in range(0, len(asins_data), batch_size):
        batch = asins_data[i:i + batch_size]
        
        for asin_record in batch:
            # Extract searchable text from ASIN record
            asin_text = ""
            if 'title' in asin_record:
                asin_text += str(asin_record['title']).lower() + " "
            if 'description' in asin_record:
                asin_text += str(asin_record['description']).lower() + " "
            if 'keywords' in asin_record:
                asin_text += str(asin_record['keywords']).lower() + " "
            
            # Calculate match score
            asin_terms = set(re.findall(r'\w+', asin_text))
            common_terms = keyword_terms.intersection(asin_terms)
            
            if common_terms:
                score = len(common_terms) / len(keyword_terms)
                matches.append({
                    'asin': asin_record.get('asin', ''),
                    'score': score,
                    'matched_terms': list(common_terms),
                    'title': asin_record.get('title', '')[:100]  # truncate for payload size
                })
    
    # Sort by score and return top matches
    matches.sort(key=lambda x: x['score'], reverse=True)
    return matches[:max_matches]

def load_all_parquet_files(s3, bucket, prefix, max_files=None):
    """Load and combine parquet files from S3 prefix with optional limit"""
    # List all parquet files
    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)
    
    parquet_files = []
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.parquet'):
                    parquet_files.append(obj['Key'])
                    if max_files and len(parquet_files) >= max_files:
                        break
        if max_files and len(parquet_files) >= max_files:
            break
    
    # Load and combine files
    dataframes = []
    for file_key in parquet_files:
        try:
            response = s3.get_object(Bucket=bucket, Key=file_key)
            df = pd.read_parquet(BytesIO(response['Body'].read()))
            dataframes.append(df)
        except Exception as e:
            print(f"Error loading {file_key}: {e}")
            continue
    
    # Combine all dataframes
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        return combined_df, parquet_files
    else:
        return pd.DataFrame(), []

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    mode = event.get('mode', 'single_file')
    
    print(f"Lambda handler called with mode: {mode}")
    print(f"Event keys: {list(event.keys())}")
    
    if mode == 'initialize':
        return initialize_job(s3, event)
    elif mode == 'process_batch':
        print("Using process_batch mode - will load ALL files")
        return process_file_batch(s3, event)
    elif mode == 'consolidate':
        return consolidate_results(s3, event)
    else:
        print(f"Using single file mode (fallback) - mode was: {mode}")
        # Legacy single file mode
        return process_single_files(s3, event)

def initialize_job(s3, event):
    """Initialize job and create file batches - NIRA pattern"""
    job_id = f"ads_keywords_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"
    
    # List ALL available files
    keywords_files = []
    asins_files = []
    
    # Get ALL keywords files
    print("Listing ALL keyword files...")
    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket='head-acceleration', Prefix='top_keywords_impressions/region_id=2/marketplace_id=4/')
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.parquet'):
                    keywords_files.append(obj['Key'])
    
    print(f"Found {len(keywords_files)} keyword files")
    
    # Get ALL ASIN files (no limit for full processing)
    print("Listing ALL ASIN files...")
    pages = paginator.paginate(Bucket='head-acceleration', Prefix='catalog_asin_keywords/marketplace_id=4/')
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.parquet'):
                    asins_files.append(obj['Key'])
    
    print(f"Found {len(asins_files)} ASIN files")
    
    # Create file batches - each batch gets ALL files for comprehensive processing
    batch_size = event.get('batch_size', 1)  # Number of batches, not files per batch
    file_batches = []
    
    # Split files across batches for parallel processing
    keywords_per_batch = len(keywords_files) // batch_size
    asins_per_batch = len(asins_files) // batch_size
    
    for i in range(batch_size):
        start_kw = i * keywords_per_batch
        end_kw = start_kw + keywords_per_batch if i < batch_size - 1 else len(keywords_files)
        
        start_asin = i * asins_per_batch  
        end_asin = start_asin + asins_per_batch if i < batch_size - 1 else len(asins_files)
        
        keywords_batch = keywords_files[start_kw:end_kw]
        asins_batch = asins_files[start_asin:end_asin]
        
        print(f"Batch {i+1}: {len(keywords_batch)} keyword files, {len(asins_batch)} ASIN files")
        
        file_batches.append({
            'mode': 'process_batch',
            'batch_id': i + 1,
            'keywords_files': keywords_batch,
            'asins_files': asins_batch,
            'job_id': job_id
        })
    
    return {
        'statusCode': 200,
        'job_id': job_id,
        'total_keywords_files': len(keywords_files),
        'total_asins_files': len(asins_files),
        'total_batches': len(file_batches),
        'file_batches': file_batches
    }

def process_file_batch(s3, event):
    """Process a batch of files - NIRA pattern with 15-minute timeout and checkpointing"""
    job_id = event['job_id']
    batch_id = event['batch_id']
    keywords_files = event['keywords_files']
    asins_files = event['asins_files']
    
    print(f"Processing batch {batch_id} with {len(keywords_files)} keyword files and {len(asins_files)} ASIN files")
    
    # Check for existing checkpoint
    checkpoint_key = f"processing/checkpoints/{job_id}/batch_{batch_id}_checkpoint.json"
    start_from_keyword = 0
    processed_companies = []
    
    try:
        checkpoint_response = s3.get_object(Bucket='head-acceleration', Key=checkpoint_key)
        checkpoint = json.loads(checkpoint_response['Body'].read())
        start_from_keyword = checkpoint.get('last_keyword_index', 0)
        processed_companies = checkpoint.get('processed_companies', [])
        print(f"Resuming from keyword index {start_from_keyword}, companies done: {processed_companies}")
    except:
        print(f"No checkpoint found, starting fresh batch {batch_id}")
    
    # Load ALL keywords from batch files
    keywords_dataframes = []
    print(f"Loading {len(keywords_files)} keyword files...")
    for i, file_key in enumerate(keywords_files):
        try:
            print(f"Loading keyword file {i+1}/{len(keywords_files)}: {file_key}")
            response = s3.get_object(Bucket='head-acceleration', Key=file_key)
            df = pd.read_parquet(BytesIO(response['Body'].read()))
            keywords_dataframes.append(df)
            print(f"Loaded {len(df)} keywords from {file_key}")
        except Exception as e:
            print(f"Error loading keywords file {file_key}: {e}")
    
    # Load ALL ASINs from batch files
    asins_dataframes = []
    print(f"Loading {len(asins_files)} ASIN files...")
    for i, file_key in enumerate(asins_files):
        try:
            print(f"Loading ASIN file {i+1}/{len(asins_files)}: {file_key}")
            response = s3.get_object(Bucket='head-acceleration', Key=file_key)
            df = pd.read_parquet(BytesIO(response['Body'].read()))
            asins_dataframes.append(df)
            print(f"Loaded {len(df)} ASINs from {file_key}")
        except Exception as e:
            print(f"Error loading ASINs file {file_key}: {e}")
    
    # Combine ALL dataframes
    keywords_df = pd.concat(keywords_dataframes, ignore_index=True) if keywords_dataframes else pd.DataFrame()
    asins_df = pd.concat(asins_dataframes, ignore_index=True) if asins_dataframes else pd.DataFrame()
    
    print(f"Combined data: {len(keywords_df)} total keywords, {len(asins_df)} total ASINs")
    
    # Discover company codes
    if 'company_code' in keywords_df.columns:
        company_codes = keywords_df['company_code'].unique().tolist()
    elif 'company_code' in asins_df.columns:
        company_codes = asins_df['company_code'].unique().tolist()
    else:
        company_codes = ['UNKNOWN']
    
    # Process each company
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    batch_results = {}
    
    # Process each company with timeout monitoring
    import time
    start_time = time.time()
    timeout_buffer = 60  # Leave 1 minute buffer before Lambda timeout
    
    for company_code in company_codes:
        if company_code in processed_companies:
            continue  # Skip already processed companies
            
        # Check remaining time
        elapsed = time.time() - start_time
        if elapsed > (900 - timeout_buffer):  # 14 minutes used
            # Save checkpoint and signal continuation needed
            checkpoint = {
                'last_keyword_index': start_from_keyword,
                'processed_companies': processed_companies,
                'batch_incomplete': True,
                'remaining_companies': [c for c in company_codes if c not in processed_companies]
            }
            s3.put_object(
                Bucket='head-acceleration',
                Key=checkpoint_key,
                Body=json.dumps(checkpoint),
                ContentType='application/json'
            )
            
            return {
                'statusCode': 206,  # Partial Content
                'job_id': job_id,
                'batch_id': batch_id,
                'batch_incomplete': True,
                'processed_companies': processed_companies,
                'remaining_companies': [c for c in company_codes if c not in processed_companies],
                'checkpoint_saved': checkpoint_key,
                'continue_processing': True
            }
        
        # Filter data by company
        if 'company_code' in keywords_df.columns:
            company_keywords = keywords_df[keywords_df['company_code'] == company_code].to_dict('records')
        else:
            company_keywords = keywords_df.to_dict('records')
            
        if 'company_code' in asins_df.columns:
            company_asins = asins_df[asins_df['company_code'] == company_code].to_dict('records')
        else:
            company_asins = asins_df.to_dict('records')
        
        # Perform ASIN matching
        keyword_matches = []
        for keyword_record in company_keywords:
            keyword = keyword_record.get('keywords', '')
            if keyword:
                matches = find_best_asin_matches(keyword, company_asins)
                keyword_matches.append({
                    'keyword': keyword,
                    'keyword_data': keyword_record,
                    'asin_matches': matches,
                    'match_count': len(matches)
                })
        
        # Store batch results
        batch_key = f"processing/batches/{job_id}/batch_{batch_id}_{company_code}_{timestamp}.json"
        s3.put_object(
            Bucket='head-acceleration',
            Key=batch_key,
            Body=json.dumps(keyword_matches, default=decimal_default),
            ContentType='application/json'
        )
        
        batch_results[company_code] = {
            'batch_s3_key': batch_key,
            'keyword_count': len(company_keywords),
            'asin_count': len(company_asins),
            'total_matches': sum(match['match_count'] for match in keyword_matches)
        }
        
        processed_companies.append(company_code)
    
    # Clean up checkpoint if batch completed
    try:
        s3.delete_object(Bucket='head-acceleration', Key=checkpoint_key)
    except:
        pass
    
    return {
        'statusCode': 200,
        'job_id': job_id,
        'batch_id': batch_id,
        'batch_results': batch_results,
        'keywords_files_processed': len(keywords_files),
        'asins_files_processed': len(asins_files),
        'companies_processed': len(company_codes)
    }

def consolidate_results(s3, event):
    """Consolidate all batch results - NIRA pattern"""
    job_id = event['job_id']
    
    # List all batch result files
    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket='head-acceleration', Prefix=f'processing/batches/{job_id}/')
    
    batch_files = []
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.json'):
                    batch_files.append(obj['Key'])
    
    # Consolidate results by company
    consolidated_results = {}
    total_keywords = 0
    total_matches = 0
    
    for batch_file in batch_files:
        try:
            response = s3.get_object(Bucket='head-acceleration', Key=batch_file)
            batch_data = json.loads(response['Body'].read())
            
            # Extract company code from filename
            company_code = batch_file.split('_')[-2]  # Extract from filename pattern
            
            if company_code not in consolidated_results:
                consolidated_results[company_code] = []
            
            consolidated_results[company_code].extend(batch_data)
            total_keywords += len(batch_data)
            total_matches += sum(item['match_count'] for item in batch_data)
            
        except Exception as e:
            print(f"Error processing batch file {batch_file}: {e}")
    
    # Store final consolidated results
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    final_results = {}
    
    for company_code, matches in consolidated_results.items():
        final_key = f"processing/final/{job_id}_final_{company_code}_{timestamp}.json"
        s3.put_object(
            Bucket='head-acceleration',
            Key=final_key,
            Body=json.dumps(matches, default=decimal_default),
            ContentType='application/json'
        )
        
        final_results[company_code] = {
            'final_s3_key': final_key,
            'total_keywords': len(matches),
            'total_matches': sum(item['match_count'] for item in matches)
        }
    
    return {
        'statusCode': 200,
        'job_id': job_id,
        'processing_complete': True,
        'batch_files_processed': len(batch_files),
        'companies_discovered': list(consolidated_results.keys()),
        'total_keywords_processed': total_keywords,
        'total_matches_found': total_matches,
        'final_results': final_results
    }

def process_single_files(s3, event):
    """Legacy single file processing for testing"""
    keywords_path = event.get('keywords_path', 'top_keywords_impressions/region_id=2/marketplace_id=4/0044_part_00.parquet')
    asins_path = event.get('asins_path', 'catalog_asin_keywords/marketplace_id=4/part-00001-f89e8f65-6330-42f2-8bc2-496bcfed50fb.c000.snappy.parquet')
    
    keywords_response = s3.get_object(Bucket='head-acceleration', Key=keywords_path)
    keywords_df = pd.read_parquet(BytesIO(keywords_response['Body'].read()))
    
    asins_response = s3.get_object(Bucket='head-acceleration', Key=asins_path)
    asins_df = pd.read_parquet(BytesIO(asins_response['Body'].read()))
    
    # Process as before...
    company_codes = keywords_df['company_code'].unique().tolist() if 'company_code' in keywords_df.columns else ['UNKNOWN']
    
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    company_results = {}
    total_keywords = 0
    total_matches = 0
    
    for company_code in company_codes:
        company_keywords = keywords_df[keywords_df['company_code'] == company_code].to_dict('records') if 'company_code' in keywords_df.columns else keywords_df.to_dict('records')
        company_asins = asins_df[asins_df['company_code'] == company_code].to_dict('records') if 'company_code' in asins_df.columns else asins_df.to_dict('records')
        
        keyword_matches = []
        for keyword_record in company_keywords:
            keyword = keyword_record.get('keywords', '')
            if keyword:
                matches = find_best_asin_matches(keyword, company_asins)
                keyword_matches.append({
                    'keyword': keyword,
                    'keyword_data': keyword_record,
                    'asin_matches': matches,
                    'match_count': len(matches)
                })
                total_matches += len(matches)
        
        matches_key = f"processing/matches/keyword_asin_matches_{company_code}_{timestamp}.json"
        s3.put_object(
            Bucket='head-acceleration',
            Key=matches_key,
            Body=json.dumps(keyword_matches, default=decimal_default),
            ContentType='application/json'
        )
        
        company_results[company_code] = {
            'matches_s3_key': matches_key,
            'keyword_count': len(company_keywords),
            'asin_count': len(company_asins),
            'total_matches': sum(match['match_count'] for match in keyword_matches)
        }
        
        total_keywords += len(company_keywords)
    
    return {
        'statusCode': 200,
        'company_results': company_results,
        'bucket_name': 'head-acceleration',
        'discovered_companies': company_codes,
        'total_companies': len(company_codes),
        'total_keywords': total_keywords,
        'total_matches': total_matches,
        'processing_complete': True
    }
